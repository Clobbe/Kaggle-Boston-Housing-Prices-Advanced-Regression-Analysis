---
title: "Kaggle Project : Boston Housing - Advanced Regression Techniques"
author: "Clobbe Norman"
date: "11/19/2017"
output: github_document
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

# 1.What are we dealing with?
Need to clean up this bulletlist later + write an introduction to the case and thank Pedro Marcelino for the inspiration and [guidance on his approach to this project](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)


1. **Understand the problem**
We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.*

2. **Univariable study**
We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.

3. **Multivariate study**
We'll try to understand how the dependent variable and independent variables relate.

4. **Basic cleaning**
We'll clean the dataset and handle the missing data, outliers and categorical variables.

5. **Test assumptions**
We'll check if our data meets the assumptions libraryd by most multivariate techniques.

## Pick up the right toolset

Meaning, importing the goodies from `tidyverse` for easy data wrangling, `ggplot2` for some nice visualization and `broom` for making sure we don't miss anything while creating our models later on.

```{r echo = FALSE, eval = FALSE}
library(ggplot2)
library(tidyverse)
library(emojifont)
load.emojifont(font = "EmojiOne.ttf")
```

```{r echo=FALSE}
df.test.raw <- read_csv('test.csv', col_names = T)
df.train.raw <- read_csv('train.csv', col_names = T)

df.test <- df.test.raw
df.train <- df.train.raw
```

Now let's have a look at the the data that we loaded into R.


```{r }
df.train %>% 
  glimpse()

```

Wow!
That's impressive - 1,460 observations and 81 variables.

From this quick overview of the dataset it seems like R interpret all the text varibles as `<chr>`, character variables. Something which will mess up later when we want to build our model and doing some visualization.

Let's fix that by turning these characters into proper factors with levels instead.


```{r }
df.test <- df.test %>% 
  unclass() %>% 
  as.data.frame()

df.train <- df.train %>% 
  unclass() %>% 
  as.data.frame()
```

Let's have a look now again at the variables.

```{r }
df.train %>%
  glimpse()

```



##Let's look at the variables
```{r }
df.train %>% 
  colnames() %>% 
  sort()
```
We did some variable evaluation, tidious but very useful. It was done in a regular [Google Spreadsheet.](https://docs.google.com/spreadsheets/d/16RMnBO7TQLbaJIiphSrcFAlJq7ewtXyojskUUsE__FM/edit?usp=sharing)

From the initial evaluation with determine that some variables are more interesting for houses and some for apartment houses. 

By following the recommendation from previous author we grouped variables into the three categories:

* `building`

* `location`

* `space`

We then also evaluated how much influence each variable would have on the price. Classifying each variable with an expectation of either `Hi`, `Med` or `Low` influence.

The problem is getting more and more tangible and now it's just about validating wether the selected variables indeed are positive correlated with an increase in price. 

```{r echo = FALSE }

#setting which columns to filter out from the main dataset
expectation.hi <- c("Neighborhood","Condition1","OverallQual",
                    "OverallCond","YearBuilt","YearRemodAdd",
                    "TotRmsAbvGrd","Fireplaces","YrSold","SalePrice")
```


To get the overview and find which variables that correlate with increased price it's convenient to do a matrix of plot who just take care of everything and give us an image with everything we're interested in. 

```{r echo = FALSE, eval= FALSE }
df.train.hi <- df.train %>% select(expectation.hi)

library(GGally)
# ggpairs(df.train.hi, cardinality_threshold = 25)
```

###Variables expected to have 'Hi' influence
![Correlation matrix plot for hi influence variables](https://dl.dropboxusercontent.com/s/bry1ahaxnyu1s2c/Screenshot%202017-11-25%2023.08.43.png)
Looks like some of the variables we expected to have high influence are positively correlated with the price. The highlighted variables are:

*  `OverallQual` (*perhaps the most fluffy variable in this dataset - the overall quality of materials and finish*)

*  `YearBuilt` (*the year when the house was built*)

*  `YearRemodAdd` (*the year when the house last was remodule*)

*  `TotRmsAbvGrd` (*the number of rooms above grade (bathrooms not included)*)


#2.What does target varible `SalePrice` look like

The scope for this project is to build a model that from a set of variables (which we're currently trying to find) will be the basis for a model which in turn can predict the price - SalePrice.


Let's find out what we know about `SalePrice`. 

```{r }
library(skimr)
df.train %>% 
  select(SalePrice) %>% 
  skim()
```

###First off - this looks perfect!

There's no zero's, meaning that the variable don't have any outliers that could later on affect our model.

Let's have a look at the distribution of `SalePrice`.

```{r}
df.train %>% 
  ggplot(aes(x = SalePrice)) +
  geom_histogram(
      aes(y = ..density..),
      fill = 'blue',
      alpha = 0.4) +
  geom_density(alpha = 0, size = 1)
```

Seems like we're dealing with a right skewed distribution, meaning it's deviating from a good ole normal distribution.

Let's find out just how skewed the distribution is.

```{r echo = TRUE}
library(moments)

df.train %>% 
  select(SalePrice) %>% 
  summarise(
      Skewness = skewness(SalePrice),
      Kurtosis = kurtosis(SalePrice)
  )
```

What does these numbers tell us?

Let's start with `Skewness`. This is simply a number for symmetry which in our case mean that the `mean(SalePrice)` is bigger than the `median(SalePrice)` and as we've already concluded this mean that our distribution is *right skewed* (as seen in plot above).

What about the `Kurtosis`? This is a metric for the *"fatness"* of the tails in our distribution. More specific what this says can be read at [MedCal.org](https://www.medcalc.org/manual/skewnesskurtosis.php)


#Let's dig deeper into the relationship

So up until now we've only looked at the relationship between `SalePrice` and numeric variables. What about the categorical variable `OverallQual`? We already know that's it's related with `SalePrice` but not how much.

```{r echo = FALSE}
library(RColorBrewer)
colors <- brewer.pal(11, 'Spectral')
```

```{r}
df.train %>% 
  ggplot(aes(x = as.factor(OverallQual), y = SalePrice, fill = as.factor(OverallQual))) + 
    geom_boxplot(outlier.alpha = 0.3,
                 outlier.stroke = 0.5) +
  
    theme(panel.grid.major = element_blank(),
          legend.position="none") +
    
  xlab('OverallQual')
```

Well, this is nothing new but now we know how the `Overall Quality` is associated with `SalePrice`


Let's dig deeper on the second categorical variable 

```{r}

df.train %>% 
  ggplot(aes(x = as.factor(YearBuilt), y = SalePrice, fill = as.factor(YearBuilt))) + 
    geom_boxplot(alpha = 0.8,
                 outlier.alpha = 0.6) +
  
    geom_smooth(aes(group=1),
                method = "lm",
                se=FALSE,
                color="black") +
  
  theme(panel.grid.major = element_blank(),
        legend.position="none",
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  
  xlab('YearBuilt')

```

####Nice colors! But what does it tell us?

As seen above the black trendline helps us to determine that there is a positive association between `SalePrice` and variable `YearBuilt`. 



###So to sum things up…

We've now found that:

* `YearBuilt`, `OverallQual`, `YearRemodAdd` and `TotRmsAbvGrd` are all variables that's correlating with `SalePrice` .


> *"But, hey! That's only 4 variables out of 81 available. Don't you miss out on a lot of potential variables that could have siginificant affect on the target?"*

In Pedro's guide he refer to that the trick for this particular case seems to be `variable selction` rather than `variable engineering`. Which seems intutively right when you think about it since we're given 81 where most of them could be variables that describe the sale price of a house (goingin through and evaluating the variables in a spreadsheet).

So far the selection of variables was based soley on intuition. Next we'll approach the problem more objectively. 


#3. ~~Intuition~~ , let's go the engineering way

To get a sense of the case that we're dealing with we approached this task in an intuitive fashion which is good, but it wasn't too objective, even thou that was our intention. Doing this I had to set a side my analytical engineering mind for a while and trust Pedro's guidance.

Luckily it's time to let the numbers do their thing and for us to approach the `varible selection` in a more objective manner. 


Let's get started!

### Heatmap and faceting plots
In order to find which variables that correlate with the target `SalePrice` we could visualize this with a correlation matrix (heatmap style) that will help us determine which varibles to choose. Along with this we're also gonna plot each variable against `SalePrice` as a scatter plots. Which we'll then faceting to get a hold of the bigger picture even more.

Let's start with a heatmap!


```{r}
library(reshape2) #loading the right package

#calculating correlations and tiding the data frame for numeric variables
df.train.cor <- df.train %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  melt()

```

In order to make it easy to filter and eventually dig deeper into the relations between the variables I'm restructuring the data frame with the `melt()` function in `reshape2`-package.

This turn out data frame into a tidy data frame with on row for each correlation between the variable and it's value, like this:

```{r}
head(df.train.cor)
```


```{r}
#plotting heatmap with ggplot2

df.train.cor %>% 
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


Oh! Cool that's what I'm talking about, but it's not to easy to decipher. 
Let's work on that by minimize the number of variables to look at through some nice piping and filtering in [dplyr](http://dplyr.tidyverse.org/). 


We're interested in reducing the noise that make it harder for us distinguish which variables are contributing to an increase on the target `SalePrice`. 


At first iteration I'm setting threshold to `0.5` and we'll see wether we need to increase or decrease it. 


```{r }
df.train.cor %>% 
  filter(value >= 0.5) %>%
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

##lägg till finlir med att ta bort variabler som inte är av intresse
```

That's much better!

With this plot it's now easier for us to decipher [the signal from the noise](https://www.amazon.com/Signal-Noise-Many-Predictions-Fail-but/dp/0143125087/ref=sr_1_1?ie=UTF8&qid=1512298650&sr=8-1&keywords=signal+and+the+noise). 


What we can see among the numerical variables now is that there's 10 that set them self from the crowed:

* `OverallQual`
* `YearBuilt`
* `YearRemodAdd`
* `TotalBsmtSF`
* `X1stFlrSF`
* `GrLivArea`
* `FullBath`
* `TotRmsAbvGrd`
* `GarageCars`
* `GarageArea`

''' städa upp och skriv rent / reflektera över om Pedros reflektioner återspeglas…

According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:

'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!
'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).
'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').
'FullBath'?? Really?
'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?
Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.
Let's proceed to the scatter plots. '''


With these variables let's label them now as either `medium` or `high` like we did in part 1. 

```{r}
interestingVariables <- c('OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','X1stFlrSF',
                          'GrLivArea','FullBath','TotRmsAbvGrd','GarageCars','GarageArea')

df.train.cor.filtered <- df.train.cor %>% 
  filter(Var1 == 'SalePrice' &
           value > 0.5 &
           value < 1) %>%
  mutate(label = ifelse(value < 0.75, yes = 'medium', no = 'high'))
```

As one can see the labels doesn't contribute that much to our case, but it was a good and valid hypothesis to try. So for now, let's continue without it.

###Health check - can we trust this?
Anyhow these 10 variables are correlated we an increase in price. Let's do some health check on each correlation to see that all of them is normally distributed or not. We'll do this with some plots in `ggplot2` but first let's structure the underlying data a bit and fit a model for each variable.

I choose to take some inspiration from a handy technique presented by [Hadley Wickham at PLOTCON 2016], (https://rpubs.com/aaronsaunders/237010).


```{r}
library(tidyr)
library(purrr)
library(broom)

df.train.filtered <- df.train %>% 
  select(interestingVariables, SalePrice) %>% 
  gather(var, value, -SalePrice) %>%
  group_by(var) %>% 
  nest()


df.train.filtered
```

Pretty!
No we have one data frame for each model containing the values for each of the variables and the target, SalePrice.

Now we can proceed to fit a linear model to each of the variables and utilize some of the magic that follows by using the `broom`-package to get some tidy coefficients which will help us proceed with our health check. 


```{r}
#creating a function to map against each variable and it's data frame.
lm_model <- function(df){
  lm(SalePrice ~ value, data = df)
}

df.train.filtered.model <- df.train.filtered %>%
    mutate(model = map(data, lm_model)) %>% 
    mutate( glance  = map(model, glance),
            rsq     = glance %>% map_dbl('r.squared'),
            tidy    = map(model, tidy),
            augment = map(model, augment)
           )
df.train.filtered.model
```

Alright!
Now we have all the underlying data that's needed in order to run our health check. Next up will be to plot the r-squared value for each variable to get a hunch of how well the model actually fit.




```{r}
df.train.filtered.model %>% 
  ggplot(aes(x = rsq, y= reorder(var, rsq))) +
  geom_point(aes(color = var)) +
  labs(y = "variable")
```

Now this justs gives us a good hint of how well each variable is to describe our target `SalePrice`, and it's pretty clear that the winner among our top 10 is `OverallQual`.

But this is not enough, let's have a look at each fitted residual to see what's going on under the hood (each point in the above plot.)

Thank you again Pedro for the guidance and steering us toward checking the distribution of the variables.

```{r}
df.train.filtered.model %>%
  unnest(augment) %>% 
```

```{r}
library(scales)
df.train.filtered.model %>%
  unnest(tidy) %>% 
  filter(term == value)
  ggplot(aes(rsq)) +
    geom_line(aes(group = var), alpha = 0.1) +
    geom_hline(yintercept = 0, color= "white", size = 2) +
    geom_smooth(se= TRUE, col = 'green') +
    scale_x_continuous(labels = comma) +
    scale_y_continuous(labels = comma) +
    ylab('Residuals') +
    ggtitle('Variable = YearBuilt')
```




```{r}
library(scales)
df.train.filtered.model %>%
  unnest(augment) %>% 
  ggplot(aes(SalePrice, .resid)) +
    geom_line(aes(group = var), alpha = 0.1) +
    geom_hline(yintercept = 0, color= "white", size = 2) +
    geom_smooth(se= TRUE, col = 'green') +
    scale_x_continuous(labels = comma) +
    scale_y_continuous(labels = comma) +
    ylab('Residuals') +
    ggtitle('Variable = YearBuilt')
```

So what we can see in this plot is that `SalePrice` is still highly skewed. In order to setup our selves for a correct analysis let's normalize our data.


```{r}


```

#5. Let's test!




